{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "b) Colab for date translation using Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtbR_OsaP38k",
        "colab_type": "text"
      },
      "source": [
        "# b) Colab for date translation using Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivZjuz56QZ8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f8dadf9d-2aa7-4987-83a7-3a8572badea5"
      },
      "source": [
        "pip install Faker\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/9d/39cc13537ba25a7819bd566d0b0cddfa5570579c194756ac3aff57256dcd/Faker-4.1.0-py3-none-any.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 348kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 358kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 368kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 378kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 389kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 399kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 409kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 419kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 430kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 440kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 450kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 460kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 471kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 481kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 491kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 501kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 512kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 522kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 532kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 542kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 552kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 563kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 573kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 583kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 593kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 604kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 614kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 624kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 634kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 645kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 655kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 665kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 675kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 686kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 696kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 706kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 716kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 727kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 737kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 747kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 757kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 768kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 778kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 788kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 798kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 808kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 819kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 829kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 839kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 849kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 860kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 870kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 880kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 890kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 901kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 911kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 921kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 931kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 942kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 952kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 962kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 972kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 983kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 993kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->Faker) (1.12.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_EwIncjQqMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c922ac75-a5bc-4975-8434-e54e0ef4074e"
      },
      "source": [
        "# !pip3 install python-utils"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement nmt_utils (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for nmt_utils\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXDv5_SGS4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1owDQoaktjDvfGV82GwVctHNhfTgSk4Bl'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n",
        "downloaded.GetContentFile('nmt_utils.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_2DJJ_EP0cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGHYLZFaQBW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d8bea6f-3d1c-4fc6-a9ac-fdc503439e4e"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 20256.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52M3fWw7Uj57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "371d6970-aaa4-49ee-87e6-14cf118e080a"
      },
      "source": [
        "dataset[:10]\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09'),\n",
              " ('10.11.19', '2019-11-10'),\n",
              " ('9/10/70', '1970-09-10'),\n",
              " ('saturday april 28 1990', '1990-04-28'),\n",
              " ('thursday january 26 1995', '1995-01-26'),\n",
              " ('monday march 7 1983', '1983-03-07'),\n",
              " ('sunday may 22 1988', '1988-05-22'),\n",
              " ('08 jul 2008', '2008-07-08'),\n",
              " ('8 sep 1999', '1999-09-08'),\n",
              " ('thursday january 1 1981', '1981-01-01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSBEEXGOUj8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "89fcb7f4-383b-4e0a-ce34-d3f41b1394bf"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Xpx4kHUp-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "6ff21afb-f44b-426b-9356-0678ec5db8fd"
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: 9 may 1998\n",
            "Target date: 1998-05-09\n",
            "\n",
            "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB6t6ixXUqA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv8_p0pxUqDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: one_step_attention\n",
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    e = densor2(e)\n",
        "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(e)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eftfBlOCUqGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 32\n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbNc33MeUqIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtiBszAxUj_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwoIySqsUkB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca1ecfbe-d070-4c20-efc6-4e314c4c0b15"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vppcszcUkEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODE HERE ### (≈2 lines)\n",
        "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
        "                    metrics=['accuracy'],\n",
        "                    loss='categorical_crossentropy')\n",
        "out\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzFBaQu_UkHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzlhxqtHVG60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a17f6d66-1811-4cdd-d4bc-cc458ff8b6dc"
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "10000/10000 [==============================] - 17s 2ms/step - loss: 16.9056 - dense_3_loss: 2.5819 - dense_3_accuracy: 0.4846 - dense_3_accuracy_1: 0.6826 - dense_3_accuracy_2: 0.3154 - dense_3_accuracy_3: 0.0875 - dense_3_accuracy_4: 0.9373 - dense_3_accuracy_5: 0.2975 - dense_3_accuracy_6: 0.0557 - dense_3_accuracy_7: 0.9735 - dense_3_accuracy_8: 0.2000 - dense_3_accuracy_9: 0.0958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f08ec4387f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np7GBXHQWjNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e0d8dace-58ea-421d-bccd-2621e1f1c9cd"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKa_NYsDW0H7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "923322c8-2edf-4993-e58a-3ae6cacfe7e7"
      },
      "source": [
        "# # serialize model to YAML\n",
        "# model_yaml = model.to_yaml()\n",
        "# with open(\"model.yaml\", \"w\") as yaml_file:\n",
        "#     yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbsNVd4eVG9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-WYIorbVG_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrC17jQcXseX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for example in EXAMPLES:\n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    \n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W92E7aI6YxvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = 'Tuesday 09 Oct 1993'\n",
        "source = string_to_int(example, Tx, human_vocab)\n",
        "source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcivYsRuVHCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6443787-7d9e-49b1-9a9f-c2186fe2fcb0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}